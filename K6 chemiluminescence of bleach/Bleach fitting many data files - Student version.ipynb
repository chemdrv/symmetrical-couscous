{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e22868b-9620-4d7b-ae92-be7d08701589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2181be-c620-4f23-9d43-1178b53fe5a9",
   "metadata": {},
   "source": [
    "# Creating a data analysis pipeline: chemiluminescence of bleach\n",
    "\n",
    "It's common to have a set of data files that all need to be analyzed in the same manner. If you have two data sets, doing them by hand might be easy. At ten, it gets repetative. But by 100 or 1000 you need to find ways to automate as much of the analysis as possible. The process of analyzing a large number of similar datasets is called a data analysis pipeline. In this notebook, you'll learn some skills to help you begin to automate your own data analysis and create a pipline.\n",
    "\n",
    "The sample dataset here is chemiluminesnce of bleach. Experimentally, solutions of bleach are combined with luminol and peroxide at varying concentrations. The resulting chemilumenesce is recorded over time. Each data set shows an initial fast rise due to the combination of the samples, then a slow decay. That decay trace can be analyzed to understand the kinetics of the reaction. The first step in the analysis is to fit each dataset to a decaying exponential and extract fit parameters.\n",
    "\n",
    "This analysis is split into two files. The first one walks you through how to fit a single data file, in order to be ready to build your pipeline. You might want to review that one if you haven't worked through it yet. This is the second notebook, which talks you though how to turn the previous notebook into a pipeline that will rapidly analyze many data files.\n",
    "\n",
    "Skills in this notebook:\n",
    "\n",
    "Skills in the next notebook:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59388a4e-d6d7-4d60-bc8a-ae52e04eadcb",
   "metadata": {},
   "source": [
    "# Building a data analysis pipeline\n",
    "\n",
    "Now that you've worked with a single data file, let's consider how we can approach many files. The first question is: just how much does it make sense to automate? We'll split it into two cases.\n",
    "\n",
    "## Case 1: You have a handful of files **or** you're really unsure of your programming ability\n",
    "\n",
    "You might want to do a kind of soft automation. The following steps were performed in the preceeding notebook:\n",
    " - [ ] Importing the first data set\n",
    " - [ ] Trimming the data\n",
    "   - [ ] Finding the peaks\n",
    "   - [ ] Slice both arrays after the last peak\n",
    " - [ ] Setting up the fit and finding the initial guess\n",
    "   - [ ] Define the fit function\n",
    "   - [ ] Pick reasonable initial guess parameters\n",
    " - [ ] Fitting the data and printing out the fit parameters\n",
    "\n",
    "Our big worry here is reproducibility. You want to structure the rest of your file such that you don't end up mixing up what you've done and what you haven't. So here's our recipe for soft automation:\n",
    "\n",
    "1. Create a markdown cell with a header for each file\n",
    "2. In that markdown cell, copy the list of steps\n",
    "3. For each file, work through the lists of steps, creating new code cells as needed. Check off the box when you've completed it.\n",
    "\n",
    "Your code cells should be repetative: each file is going through the same set of steps. The markdown cell is key here: it's going to ensure that you've completed each step. It's also going to allow you to open up your file on another day and see where you left off.\n",
    "\n",
    "The one thing that we need to add is a way to collect all the data, and then a nice way to display it when we're done. There are two cells below. The first shows you how to make Python lists and append numbers, which is how you'll handle each of your fit parameters. The second uses the `os.path` library to access the filename and store it in a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb1a9de-a2af-4494-91e5-e856bf7daa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.23, 4700]\n"
     ]
    }
   ],
   "source": [
    "list_of_numbers = [] #initialize the list somewhere near the top of your notebook\n",
    "list_of_numbers.append(1) #each time you get a fit parameter, you can append it to your list\n",
    "list_of_numbers.append(1.23)\n",
    "list_of_numbers.append(4700)\n",
    "print(list_of_numbers)\n",
    "# Of course you'll need one list for each fit parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d061a4af-9b81-4537-81fc-a1f40d8e3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_CL_T2.csv', 'A_CL_T1.csv', 'A_CL_T3.csv']\n"
     ]
    }
   ],
   "source": [
    "files = [] #initialize this list where you initialize the ones for fit parameters\n",
    "\n",
    "#pick a file for analysis\n",
    "filename = \"bleach/A_CL_T2.csv\"\n",
    "#load the file to do your analysis\n",
    "dataX_raw, dataY_raw = np.loadtxt(filename, skiprows = 21, usecols = (2,3), delimiter = ',', unpack = True)\n",
    "#grab out the important part of the filename and append it to your list\n",
    "files.append(os.path.basename(filename))\n",
    "\n",
    "#continue with the rest of your analysis\n",
    "#\n",
    "#\n",
    "\n",
    "# Appending a couple more filenames\n",
    "filename = \"bleach/A_CL_T1.csv\"\n",
    "files.append(os.path.basename(filename))\n",
    "filename = \"bleach/A_CL_T3.csv\"\n",
    "files.append(os.path.basename(filename))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1da12-30a6-4eaf-82ba-00a34e89ad3c",
   "metadata": {},
   "source": [
    "After you've fit all your files, it'd be nice to display the filenames and fit parameters in a neat table. We can accomplish that by using a new library: Pandas. This library extends Numpy arrays to work with a wide variety of data types and shapes. The Pandas library has a lovely tutorial titled \"10 minutes to pandas\" that shows the basic functionality, located here: https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "\n",
    "The cell below shows you how to take the two sample lists from above, assemble them into a dataframe, and display the results. Use this as an example to make a dataframe that contains your filenames and all three of your fit parameters for each file.\n",
    "\n",
    "Your dataframe can also be used to access, plot, and manipulate your fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a579edf6-5fa3-4957-8047-22fc0ebe62a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_CL_T2.csv</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_CL_T1.csv</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_CL_T3.csv</td>\n",
       "      <td>4700.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          File        A\n",
       "0  A_CL_T2.csv     1.00\n",
       "1  A_CL_T1.csv     1.23\n",
       "2  A_CL_T3.csv  4700.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"File\":files, \"A\":list_of_numbers})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d7248-f025-4bcf-9cbf-8bed17968822",
   "metadata": {},
   "source": [
    "## Case 2: You have may files and want to automate the process\n",
    "\n",
    "In the case where you have a large number of files, you want to try to automate the work as much as possible. So far, the user inputs have been: finding the index, setting the initial parameters, and manually typing in each step. There are a lot of options here.\n",
    "\n",
    "Much of what we did in the previous notebook was structured to make this easier. For example, we found an algorithmic way to guess initial parameters. Now we need to learn how to grab out all of our files and repeat the analysis on the whole group. As we go, we should grab filenames and fit parameters in lists, so that we can make a pretty dataframe at the end.\n",
    "\n",
    "Here's our list of steps:\n",
    " - [ ] Grab all the filenames\n",
    " - [ ] Make a loop to iterate over all filenames\n",
    " - [ ] Add in the steps from our soft automation\n",
    " - [ ] Plan for issues\n",
    " - [ ] Add in lists for fit parameters and collect them in a dataframe\n",
    "\n",
    "### Grab all the filenames\n",
    "\n",
    "We're going to use two libraries: `os.path` (https://docs.python.org/3/library/os.path.html) and glob (https://docs.python.org/3/library/glob.html). `os.path` contains basic functions for handling directory structures and filenames. `glob` let's us find many files that fit a particular pattern for their names.\n",
    "\n",
    "Your data should be in a folder within your current working directory. Hopefully it's also named in some consistant way. The wildcard character (`*`) can be use to to replace a string of text. `os.path.join('name_of_folder_here', \"*.csv\")` would create a path to all CSV files in the folder specified in the first argument. `glob.glob()` then finds all files that fit that path. Fill in the cell below to create a path to your datafiles, find them all with `glob.glob()`, and print the list of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc90ab-f952-429f-9904-8bddc9c66452",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = os.path.join(\n",
    "files = glob.glob(\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc785d2a-fa71-4e18-9b0f-75f7b5e9af82",
   "metadata": {},
   "source": [
    "### Make a loop to iterate over all filenames\n",
    "\n",
    " - [x] Grab all the filenames\n",
    " - [ ] Make a loop to iterate over all filenames\n",
    " - [ ] Add in the steps from our soft automation\n",
    " - [ ] Plan for issues\n",
    " - [ ] Add in lists for fit parameters\n",
    " - [ ] Make a nice dataframe of the parameters\n",
    " \n",
    " A `for` loop can be used to iterate over these filenames. Refer to this item in the Python Tutorial if you need to learn the syntax for a `for` loop: https://docs.python.org/3/tutorial/controlflow.html#for-statements\n",
    " \n",
    " Use the cell below to make a `for` loop over your list of files. Have your loop print each filename, to show you that you've got it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062e962-eae9-4325-a6d0-2b830788a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44e6167e-e1e6-4f6d-83b0-a93cd2f43c52",
   "metadata": {},
   "source": [
    "### Add in the steps from our soft automation\n",
    "\n",
    " - [x] Grab all the filenames\n",
    " - [x] Make a loop to iterate over all filenames\n",
    " - [ ] Add in the steps from our soft automation\n",
    " - [ ] Plan for issues\n",
    " - [ ] Add in lists for fit parameters and collect them in a dataframe\n",
    " \n",
    "Now you can go back to your previous notebook or your soft automation above. Collecte the steps of fitting and put them inside your loop. Don't forget to define your model function somewhere. But before you run it...\n",
    "\n",
    "### Plan for issues\n",
    "\n",
    " - [x] Grab all the filenames\n",
    " - [x] Make a loop to iterate over all filenames\n",
    " - [x] Add in the steps from our soft automation\n",
    " - [ ] Plan for issues\n",
    " - [ ] Add in lists for fit parameters and collect them in a dataframe\n",
    " \n",
    "It's quite possible that one or more files won't find any peaks. If that happens, your loop will exit with an error. Instead, we can add in an `if`/`else` statement to handle that case smoothly. You can write something like this:\n",
    "```\n",
    "for file in files:\n",
    "    open the file\n",
    "    find the peaks\n",
    "    if len(peaks)>0:\n",
    "        do all the fitting steps\n",
    "    else:\n",
    "        print(f\"No peaks found for file {file}\")\n",
    "```\n",
    "\n",
    "### Add in lists for fit parameters and collect them in a dataframe\n",
    "\n",
    " - [x] Grab all the filenames\n",
    " - [x] Make a loop to iterate over all filenames\n",
    " - [x] Add in the steps from our soft automation\n",
    " - [x] Plan for issues\n",
    " - [ ] Add in lists for fit parameters and collect them in a dataframe\n",
    " \n",
    " Finally, initialize empty lists ahead of your loop for each parameter and the filenames. Then, at the end of the fitting section, append the fit parameters and current filename to your lists. After the `for` loop, collect your lists into a dataframe. You can print that dataframe for futher analysis in this notebook, or save it to a text file to access from another notebook. This page shows you how to write dataframes to just about any format your heart desires: https://pandas.pydata.org/docs/user_guide/io.html\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97858a-2f7a-43c5-9e99-340f1eb93d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
